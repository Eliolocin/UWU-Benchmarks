## UWU Benchmarks
### NVCS (N-Gram Vector Cosine Similarity)
- See how similar two texts' writing styles are using Character N-Grams (sequence of characters and punctuations). 
- Converts the Sample Dialogues and the Generated Response into Character N-Gram Vectors in which Cosine Similarity will be calculated. 
 - 1.0 means exactly the same, 0.0 means no similarity at all. The higher the NVCS, the better the LLM can copy the Chatbot's writing style from Sample Dialogues -> Generated Responses.
 
$$ nvcs(D, R) = \frac{\sum{i=1}^{n}D{i}R{i}}{\sqrt{\sum{i=1}^{n}D{i}^{2}}\cdot\sqrt{\Sigma{i=1}^{n}R_{i}^{2}}} $$
<div align="center">
D = User's Sample Dialogues as Vectorized Character N-Gram <br>
R = LLM's Generated Responses as Vectorized Character N-Gram
</div>

 ### ERTD (English Readability Transfer Difference)
- See how different the English readability scores are between two texts
- Absolute value of the difference between the English Readability of the Sample Dialogues and English Readability of Generated Responses. Formula to be used is Flesch-Kincaid Reading Ease.
- The lower the ERTD, the better the LLM can adapt the Text Difficulty seen in the Userâ€™s Sample Dialogues into its Generated Responses
- Value is clamped to 0 to 100 for edge cases where Flesch-Kincaid returns values outside of expected boundary

$$Fk_t=206.835 - 1.015 * (ASL_t) - 84.6 * (ASW_t)$$
$$Er_t= \begin{cases}
100, & \text{if } Fk_t > 100 \\
Fk_t, & \text{if } Fk_t > 0 \\
0, & \text{otherwise}
\end{cases}$$
<div align="center">
Fk<sub>t</sub> = Flesch-Kincaid Reading Ease of Text t<br>
Er<sub>t</sub> = English Readability of Text t<br>
ASL = Total Words in Text / Total Sentences in Text t<br>
ASW = Total Syllables in Text / Total Words in Text t
</div>
<br>

$$ertd(Er_d,Er_r) = |Er_d - Er_r|$$
<div align="center">
Er<sub>d</sub> = English Readability of User's Sample Dialogues <br>
Er<sub>r</sub> = English Readability of LLM's Generated Responses
</div>

### JERA (Judgement Evaluation on Roleplay Ability)
- Using LLM-as-Judge (which is popularly used in other benchmark tools), evaluate how well the LLM Roleplays in general, using a 10-point Likert Scale questionnaire consisting of 10 items each for 3 different metrics, (1) Immersivity, (2) Consistency, (3) and Creativity:
- **Immersivity**: Evaluates how well the LLM can immerse the user in the roleplay, making the experience enjoyable and engaging. This includes staying in character, reacting appropriately to scenarios, and maintaining a believable and captivating narrative.
- **Consistency**: Assesses the LLM's ability to stay true to the character throughout the roleplay. This involves maintaining the character's personality, background, mannerisms, and speech style without breaking character or deviating from the established persona.
- **Creativity:** Measures the LLM's ability to generate unique and dynamic responses, avoiding redundancy. This includes creating interesting dialogue, actions, and scenarios that keep the roleplay fresh and engaging.
- The Judge LLM is provided the Attribute List and Sample Dialogues as well, to help with the evaluation.
- Judge LLM answers a questionnaire as if they were a human evaluating and testing the UWU Chatbot for Roleplaying. JERA is the total score returned.
- The higher the JERA score, the better the LLM can Roleplay in general as perceived by an outside evaluator. Total score returned is 300.

$$jera(Rp) = Imm_{\text{Rp}}+Con_{\text{Rp}}+Cre_{\text{Rp}}$$
<div align="center">
Rp = LLM's Generated Roleplay Scene<br>
Imm<sub>Rp</sub> = LLM's evaluated Immersivity score during Roleplay<br>
Con<sub>Rp</sub> = LLM's evaluated Consistency score during Roleplay<br>
Cre<sub>Rp</sub> = LLM's evaluated Creativity score during Roleplay<br>
</div>




### ALMP (Attribute List Match Percentage)
- See how much percent of the Attribute List can be observed in the Roleplay Scene generated by the LLM
- Looks for matches between the set of attributes observed in the Attribute List and the set of attributes observed in the Roleplay Scene. Observations are found through an Extraction model and an Inference model working together.
- The higher the ALMP, the better the LLM applied the User's Attribute List into its roleplaying.
- Value returned is 0.0-1.0.

$$
\text{almp(A,R)} = \frac{M}{\text{count(A)}}
$$
$$M = \sum_{i=1}^{A} \text{Match}(a_i, R)$$
<div align="center">
M = Total number of matched attributes between Attribute List set A and Roleplay Scene set R<br>
Match(a<sub>i</sub>, R) = Function to determine if the attribute a<sub>i</sub> matches any attribute in R
</div>
<br>

$$
\text{Match}(a_i, R) = 
\begin{cases} 
1 & \text{if } a_i \in R \quad  \\
1 & \text{if } \text{FuzzyRatio}(a_i, r) > 85, \quad \forall r \in R \quad  \\
1 & \text{if } \text{SynSet}(a_i) \cap \text{SynSet}(r) \neq \emptyset, \quad \forall r \in R \quad  \\
0 & \text{otherwise}
\end{cases}
$$
<div align="center">
Match(a<sub>i</sub>, R) = Determines if the attribute a<sub>i</sub> from the Attribute List matches any attribute in the Roleplay Scene set R.<br>
a<sub>i</sub> = An attribute from the Attribute List.<br>
R = Set of attributes extracted from the Roleplay Scene.<br>
FuzzyRatio(a<sub>i</sub>, r) = Levenshtein-based similarity ratio between attributes a<sub>i</sub> and r.<br>
SynSet(a<sub>i</sub>) = Set of synonyms for attribute a<sub>i</sub> generated using WordNet.<br>
r = An attribute from the Roleplay Scene set R.
</div>
<br>

