{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data into Benchmark Tool\n",
    "\n",
    "This section initializes the UWU-Benchmarks evaluation framework by loading the configuration settings and character definitions. The configuration file contains API keys, LLM selection, and benchmark parameters, while character files define the roleplay personas to be evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datahelper import DataHelper\n",
    "\n",
    "# Load the config file\n",
    "config = DataHelper.load_config('./config.ini') \n",
    "\n",
    "# Load all characters from directory into a Character Object array\n",
    "characters = DataHelper.load_character_directory('./characters', config) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Roleplay Scene Generation\n",
    "\n",
    "This phase uses the selected LLM to generate interactive roleplay scenes for each character. The system provides sample dialogues and character attributes as context, then generates multi-turn conversations between the user and AI character to evaluate roleplay performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llms.gpt import GPT\n",
    "from llms.claude import Claude\n",
    "from llms.gemini import Gemini\n",
    "from llms.novelai import NovelAI\n",
    "\n",
    "# For each character, produce roleplay scenes\n",
    "if config.test_llm.startswith('gpt'):\n",
    "    for character in characters:\n",
    "        character.rpscene = GPT.generate_roleplay(character, config)\n",
    "elif config.test_llm.startswith('claude'):\n",
    "    for character in characters:\n",
    "        character.rpscene = Claude.generate_roleplay(character, config)\n",
    "elif config.test_llm.startswith('gemini'):\n",
    "    for character in characters:\n",
    "        character.rpscene = Gemini.generate_roleplay(character, config)\n",
    "else:\n",
    "    for character in characters:\n",
    "        character.rpscene = await NovelAI.generate_roleplay(character, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Benchmark Evaluation\n",
    "\n",
    "This final stage applies all four benchmark algorithms (NVCS, ERTD, JERA, ALMP) to evaluate the generated roleplay content. Each algorithm measures different aspects of roleplay quality, and results are combined into comprehensive UWU scores across all test characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from algorithms.nvcs import NVCS\n",
    "from algorithms.ertd import ERTD\n",
    "from algorithms.jera import JERA\n",
    "from algorithms.almp import ALMP\n",
    "from algorithms.uwu import UWU\n",
    "from datahelper import DataHelper\n",
    "\n",
    "import datetime\n",
    "import statistics as st\n",
    "\n",
    "# Initialize benchmark algorithms\n",
    "nvcs, ertd, jera, almp, uwu = NVCS(), ERTD(), JERA(), ALMP(), UWU()\n",
    "nvcs_scores, ertd_scores, jera_scores, almp_scores, uwu_scores, character_evals = [], [], [], [], [], []\n",
    "\n",
    "# Use generated responses to calculate benchmark scores\n",
    "characters = DataHelper.load_generations_directory('./generations')\n",
    "config = DataHelper.load_config('./config.ini') \n",
    "\n",
    "# For each LLM, gather the average benchmark scores based on all Characters\n",
    "for character in characters:\n",
    "\n",
    "    # Get all test character's utterances from sample dialogues as single string, separated by spaces, excluding the character's name\n",
    "    charaOnlySampleDialogues = \" \".join([dialogue.split(\": \", 1)[1] for dialogue in character.smpdialogues if dialogue.startswith(f\"{character.name}:\")]) \n",
    "    # Get all test character's utterances from generated responses as single string, separated by spaces, excluding the character's name\n",
    "    charaOnlyGeneratedResponses = \" \".join([response.split(\": \", 1)[1] for response in character.rpscene if response.startswith(f\"{character.name}:\")]) \n",
    "    # Get all roleplay dialogue between the test character and user character as single string, separated by newlines, including character's name\n",
    "    fullRoleplay = \"\\n\".join(character.rpscene) \n",
    "\n",
    "    print(f\"========================== Benchmarking using {character.name}... ==========================\\n\")\n",
    "    \n",
    "    nvcs_score = nvcs.calculate_nvcs(config.ngram, charaOnlySampleDialogues, charaOnlyGeneratedResponses) # Return calculated NVCS as Float value\n",
    "    print(f\"- NVCS Score: {nvcs_score:.2f} out of 1.0\\n\")\n",
    "\n",
    "    ertd_score = ertd.calculate_ertd(charaOnlySampleDialogues, charaOnlyGeneratedResponses)\n",
    "    print(f\"- ERTD Score: {ertd_score:.2f} out of 100\\n\")\n",
    "    \n",
    "    jera_score = jera.calculate_jera(character.name, character.attributes, character.smpdialogues, fullRoleplay, config)\n",
    "    print(f\"- JERA Score: {jera_score} out of 300\\n\")\n",
    "\n",
    "    almp_score = almp.calculate_almp(character.attributes, charaOnlyGeneratedResponses, config)\n",
    "    print(f\"- ALMP Score: {almp_score:.2f} out of 1.0\\n\")\n",
    "\n",
    "    uwu_score = uwu.calculate_uwu_score(nvcs_score, ertd_score, jera_score, almp_score)\n",
    "    print(f\"- UwU Score: {uwu_score:.2f} out of 100\\n\")\n",
    "\n",
    "    nvcs_scores.append(nvcs_score)\n",
    "    ertd_scores.append(ertd_score)\n",
    "    jera_scores.append(jera_score)\n",
    "    almp_scores.append(almp_score)\n",
    "    uwu_scores.append(uwu_score)\n",
    "\n",
    "    # Save scores unto character_evals\n",
    "    character_evals.append(\n",
    "        {\n",
    "            'character': {\n",
    "                'scores': {\n",
    "                    'nvcs': nvcs_score,\n",
    "                    'ertd': ertd_score,\n",
    "                    'jera': jera_score,\n",
    "                    'almp': almp_score,\n",
    "                    'uwu': uwu_score\n",
    "                },\n",
    "                'name': character.name\n",
    "            }\n",
    "        }\n",
    "            \n",
    "    )\n",
    "\n",
    "print(f\"========================== Benchmarking all characters accomplished, now saving... ==========================\")\n",
    "\n",
    "# Save Benchmark results to file\n",
    "benchmark_results = {\n",
    "    'date_generated': datetime.datetime.now().strftime(\"%Y%m%d_%H%M\"),\n",
    "    'results': {\n",
    "        'avg_scores': { # Average results\n",
    "            'nvcs': st.mean(nvcs_scores),\n",
    "            'ertd': st.mean(ertd_scores),\n",
    "            'jera': st.mean(jera_scores),\n",
    "            'almp': st.mean(almp_scores),\n",
    "            'uwu': st.mean(uwu_scores)\n",
    "        },\n",
    "        'max_scores': { # Best results\n",
    "            'nvcs': max(nvcs_scores),\n",
    "            'ertd': max(ertd_scores),\n",
    "            'jera': max(jera_scores),\n",
    "            'almp': max(almp_scores),\n",
    "            'uwu': max(uwu_scores)\n",
    "        },\n",
    "        'min_scores': { # Worst results\n",
    "            'nvcs': min(nvcs_scores),\n",
    "            'ertd': min(ertd_scores),\n",
    "            'jera': min(jera_scores),\n",
    "            'almp': min(almp_scores),\n",
    "            'uwu': min(uwu_scores)\n",
    "        },\n",
    "        'std_devs': { # Standard Deviations of results\n",
    "            'nvcs': st.stdev(nvcs_scores) if len(nvcs_scores) > 1 else 0,\n",
    "            'ertd': st.stdev(ertd_scores) if len(ertd_scores) > 1 else 0,\n",
    "            'jera': st.stdev(jera_scores) if len(jera_scores) > 1 else 0,\n",
    "            'almp': st.stdev(almp_scores) if len(almp_scores) > 1 else 0,\n",
    "            'uwu': st.stdev(uwu_scores) if len(uwu_scores) > 1 else 0\n",
    "        }\n",
    "    },\n",
    "    'config': {\n",
    "        'ngram': config.ngram,\n",
    "        'smp_dialogues_max': config.smp_dialogues_max,\n",
    "        'test_llm': config.test_llm,\n",
    "        'rp_turns': config.rp_turns,\n",
    "        'user_name': config.user_name\n",
    "    },\n",
    "    'characount': len(characters),\n",
    "    'characters_used': character_evals\n",
    "}\n",
    "\n",
    "DataHelper.save_results(benchmark_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
